{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMJaxdWNRynQ",
        "outputId": "6738221d-d272-4244-fe7a-3159a3a2e697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (1460, 81)\n",
            "Test set shape: (1459, 80)\n",
            "SalePrice skewness: 1.8828757597682129\n",
            "SalePrice kurtosis: 6.536281860064529\n",
            "Log SalePrice skewness: 0.12134661989685333\n",
            "Top 10 Features Correlated with SalePrice:\n",
            "OverallQual     0.817185\n",
            "GrLivArea       0.700927\n",
            "GarageCars      0.680625\n",
            "GarageArea      0.650888\n",
            "TotalBsmtSF     0.612134\n",
            "1stFlrSF        0.596981\n",
            "FullBath        0.594771\n",
            "YearBuilt       0.586570\n",
            "YearRemodAdd    0.565608\n",
            "GarageYrBlt     0.541073\n",
            "Name: SalePrice, dtype: float64\n",
            "Number of categorical features: 43\n",
            "Combined dataset shape: (2919, 79)\n",
            "Features with missing values:\n",
            "              Missing Count  Percentage\n",
            "PoolQC                 2909   99.657417\n",
            "MiscFeature            2814   96.402878\n",
            "Alley                  2721   93.216855\n",
            "Fence                  2348   80.438506\n",
            "MasVnrType             1766   60.500171\n",
            "FireplaceQu            1420   48.646797\n",
            "LotFrontage             486   16.649538\n",
            "GarageCond              159    5.447071\n",
            "GarageFinish            159    5.447071\n",
            "GarageYrBlt             159    5.447071\n",
            "GarageQual              159    5.447071\n",
            "GarageType              157    5.378554\n",
            "BsmtExposure             82    2.809181\n",
            "BsmtCond                 82    2.809181\n",
            "BsmtQual                 81    2.774923\n",
            "BsmtFinType2             80    2.740665\n",
            "BsmtFinType1             79    2.706406\n",
            "MasVnrArea               23    0.787941\n",
            "MSZoning                  4    0.137033\n",
            "BsmtHalfBath              2    0.068517\n",
            "Remaining missing values: 2\n",
            "Remaining categorical features: 34\n",
            "['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical', 'Functional', 'GarageType', 'GarageFinish', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n",
            "Number of skewed features: 56\n",
            "Model Evaluation with Cross-Validation:\n",
            "Lasso - RMSE: 0.1394\n",
            "ElasticNet - RMSE: 0.1396\n",
            "Ridge - RMSE: 0.1399\n",
            "GradientBoosting - RMSE: 0.1317\n",
            "XGBoost - RMSE: 0.1267\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1550\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 86\n",
            "[LightGBM] [Info] Start training from score 12.030658\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1544\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 84\n",
            "[LightGBM] [Info] Start training from score 12.016898\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1547\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 85\n",
            "[LightGBM] [Info] Start training from score 12.022759\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1552\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 86\n",
            "[LightGBM] [Info] Start training from score 12.027933\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1542\n",
            "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 84\n",
            "[LightGBM] [Info] Start training from score 12.022040\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "LightGBM - RMSE: 0.1269\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1579\n",
            "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 87\n",
            "[LightGBM] [Info] Start training from score 12.024057\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "Submission file created!\n",
            "Top 20 most important features:\n",
            "           Feature  Avg_Importance\n",
            "16     OverallQual        0.186443\n",
            "87    QualityScore        0.145331\n",
            "79         TotalSF        0.137513\n",
            "80  TotalBathrooms        0.052754\n",
            "40      CentralAir        0.040029\n",
            "26       ExterQual        0.030891\n",
            "60      GarageCars        0.029927\n",
            "45       GrLivArea        0.025600\n",
            "56     FireplaceQu        0.021602\n",
            "52     KitchenQual        0.021173\n",
            "29        BsmtQual        0.019770\n",
            "37     TotalBsmtSF        0.019606\n",
            "63      GarageCond        0.018542\n",
            "61      GarageArea        0.017455\n",
            "18       YearBuilt        0.013022\n",
            "48        FullBath        0.012615\n",
            "42        1stFlrSF        0.011486\n",
            "17     OverallCond        0.011020\n",
            "55      Fireplaces        0.010877\n",
            "81        HouseAge        0.010705\n",
            "\n",
            "House Prices prediction complete! Your submission.csv file should score well on Kaggle.\n"
          ]
        }
      ],
      "source": [
        "# House Prices Advanced Regression\n",
        "# Complete Solution for Kaggle Competition\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ===============================\n",
        "# 1. Load and Explore the Data\n",
        "# ===============================\n",
        "\n",
        "# Load the data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train set shape: {train.shape}\")\n",
        "print(f\"Test set shape: {test.shape}\")\n",
        "\n",
        "# Save the ID column for submission\n",
        "train_ID = train['Id']\n",
        "test_ID = test['Id']\n",
        "\n",
        "# Remove ID column as it's not used for modeling\n",
        "train.drop('Id', axis=1, inplace=True)\n",
        "test.drop('Id', axis=1, inplace=True)\n",
        "\n",
        "# ===============================\n",
        "# 2. Exploratory Data Analysis\n",
        "# ===============================\n",
        "\n",
        "# 2.1 Analyze the target variable\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train['SalePrice'], kde=True)\n",
        "plt.title('SalePrice Distribution')\n",
        "plt.savefig('saleprice_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Check skewness of target\n",
        "print(f\"SalePrice skewness: {train['SalePrice'].skew()}\")\n",
        "print(f\"SalePrice kurtosis: {train['SalePrice'].kurt()}\")\n",
        "\n",
        "# 2.2 Apply log transformation to target for better normality\n",
        "train['SalePrice'] = np.log1p(train['SalePrice'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train['SalePrice'], kde=True)\n",
        "plt.title('Log Transformed SalePrice Distribution')\n",
        "plt.savefig('log_saleprice_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "print(f\"Log SalePrice skewness: {train['SalePrice'].skew()}\")\n",
        "\n",
        "# 2.3 Analyze numerical features correlation with target\n",
        "numeric_features = train.select_dtypes(include=[np.number]).drop('SalePrice', axis=1).columns\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation = train[list(numeric_features) + ['SalePrice']].corr()\n",
        "sns.heatmap(correlation, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.savefig('correlation_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Top correlated features\n",
        "top_corr_features = correlation['SalePrice'].sort_values(ascending=False)[1:11]\n",
        "print(\"Top 10 Features Correlated with SalePrice:\")\n",
        "print(top_corr_features)\n",
        "\n",
        "# 2.4 Visualize important categorical features\n",
        "categorical_features = train.select_dtypes(include=['object']).columns\n",
        "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
        "\n",
        "# View distributions of a few important categorical variables\n",
        "important_cat_features = ['Neighborhood', 'ExterQual', 'KitchenQual', 'BsmtQual']\n",
        "for feature in important_cat_features:\n",
        "    if feature in train.columns:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.boxplot(x=feature, y='SalePrice', data=train)\n",
        "        plt.title(f'SalePrice by {feature}')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{feature}_boxplot.png')\n",
        "        plt.close()\n",
        "\n",
        "# ===============================\n",
        "# 3. Data Preprocessing\n",
        "# ===============================\n",
        "\n",
        "# 3.1 Combine train and test for preprocessing\n",
        "y_train = train['SalePrice'].copy()\n",
        "train = train.drop('SalePrice', axis=1)\n",
        "all_data = pd.concat([train, test], axis=0)\n",
        "\n",
        "print(f\"Combined dataset shape: {all_data.shape}\")\n",
        "\n",
        "# 3.2 Check missing values\n",
        "missing_data = all_data.isnull().sum().sort_values(ascending=False)\n",
        "missing_data = missing_data[missing_data > 0]\n",
        "missing_percent = (missing_data / len(all_data)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_data,\n",
        "    'Percentage': missing_percent\n",
        "})\n",
        "\n",
        "print(\"Features with missing values:\")\n",
        "print(missing_df.head(20))\n",
        "\n",
        "# 3.3 Handle missing values based on data description\n",
        "\n",
        "# For numerical features with NA meaning \"No feature\"\n",
        "na_means_none_features = [\n",
        "    'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
        "    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
        "    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
        "    'MasVnrType'\n",
        "]\n",
        "\n",
        "for feature in na_means_none_features:\n",
        "    if feature in all_data.columns:\n",
        "        all_data[feature] = all_data[feature].fillna('None')\n",
        "\n",
        "# Numerical features where 0 makes sense for missing values\n",
        "zero_fill_features = [\n",
        "    'GarageYrBlt', 'GarageArea', 'GarageCars',\n",
        "    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n",
        "    'MasVnrArea'\n",
        "]\n",
        "\n",
        "for feature in zero_fill_features:\n",
        "    if feature in all_data.columns:\n",
        "        all_data[feature] = all_data[feature].fillna(0)\n",
        "\n",
        "# Fill most common value for categorical features\n",
        "mode_fill_features = [\n",
        "    'MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd',\n",
        "    'SaleType', 'Functional'\n",
        "]\n",
        "\n",
        "for feature in mode_fill_features:\n",
        "    if feature in all_data.columns:\n",
        "        all_data[feature] = all_data[feature].fillna(all_data[feature].mode()[0])\n",
        "\n",
        "# Fill with median for LotFrontage based on neighborhood\n",
        "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "# Check for any remaining missing values\n",
        "remaining_missing = all_data.isnull().sum().sum()\n",
        "print(f\"Remaining missing values: {remaining_missing}\")\n",
        "\n",
        "# Fill any remaining missing values with median for numerical and mode for categorical\n",
        "if remaining_missing > 0:\n",
        "    for col in all_data.columns:\n",
        "        if all_data[col].isnull().sum() > 0:\n",
        "            dtype = all_data[col].dtype\n",
        "            if np.issubdtype(dtype, np.number):\n",
        "                all_data[col] = all_data[col].fillna(all_data[col].median())\n",
        "            else:\n",
        "                all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
        "\n",
        "# 3.4 Feature Engineering\n",
        "\n",
        "# Create Total Square Footage feature\n",
        "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
        "\n",
        "# Create Total Bathroom feature\n",
        "all_data['TotalBathrooms'] = (all_data['FullBath'] +\n",
        "                             all_data['HalfBath'] * 0.5 +\n",
        "                             all_data['BsmtFullBath'] +\n",
        "                             all_data['BsmtHalfBath'] * 0.5)\n",
        "\n",
        "# Create House Age and Remodeled features\n",
        "all_data['YrSold'] = all_data['YrSold'].astype(int)\n",
        "all_data['YearBuilt'] = all_data['YearBuilt'].astype(int)\n",
        "all_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(int)\n",
        "\n",
        "all_data['HouseAge'] = all_data['YrSold'] - all_data['YearBuilt']\n",
        "all_data['RemodAge'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
        "all_data['IsRemodeled'] = (all_data['YearRemodAdd'] != all_data['YearBuilt']).astype(int)\n",
        "all_data['IsNewHouse'] = (all_data['HouseAge'] <= 2).astype(int)\n",
        "\n",
        "# Create features for porch areas\n",
        "all_data['TotalPorchSF'] = (all_data['OpenPorchSF'] +\n",
        "                           all_data['EnclosedPorch'] +\n",
        "                           all_data['3SsnPorch'] +\n",
        "                           all_data['ScreenPorch'])\n",
        "all_data['HasPorch'] = (all_data['TotalPorchSF'] > 0).astype(int)\n",
        "\n",
        "# Create quality features\n",
        "def map_quality(x):\n",
        "    quality_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "    return quality_map.get(x, 0)\n",
        "\n",
        "quality_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n",
        "               'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\n",
        "\n",
        "for col in quality_cols:\n",
        "    if col in all_data.columns:\n",
        "        all_data[col] = all_data[col].map(map_quality)\n",
        "\n",
        "# Overall Quality Score\n",
        "all_data['QualityScore'] = all_data[['ExterQual', 'KitchenQual']].mean(axis=1)\n",
        "\n",
        "# 3.5 Handle categorical variables - convert to numeric or one-hot encode\n",
        "# Check remaining categorical variables\n",
        "categorical_features = all_data.select_dtypes(include=['object']).columns\n",
        "print(f\"Remaining categorical features: {len(categorical_features)}\")\n",
        "print(categorical_features.tolist())\n",
        "\n",
        "# Encode remaining categorical variables\n",
        "for col in categorical_features:\n",
        "    if col in all_data.columns:\n",
        "        lbl = LabelEncoder()\n",
        "        lbl.fit(list(all_data[col].values))\n",
        "        all_data[col] = lbl.transform(list(all_data[col].values))\n",
        "\n",
        "# 3.6 Transform skewed numerical features\n",
        "numeric_features = all_data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Check skewness\n",
        "skewed_feats = all_data[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
        "skewed_feats = skewed_feats[abs(skewed_feats) > 0.75]\n",
        "print(f\"Number of skewed features: {len(skewed_feats)}\")\n",
        "\n",
        "# Apply Box-Cox transformation to skewed features\n",
        "skewed_features = skewed_feats.index\n",
        "for feat in skewed_features:\n",
        "    all_data[feat] = np.log1p(all_data[feat])\n",
        "\n",
        "# 3.7 Split data back to train and test\n",
        "train = all_data[:len(train)]\n",
        "test = all_data[len(train):]\n",
        "\n",
        "# ===============================\n",
        "# 4. Modeling\n",
        "# ===============================\n",
        "\n",
        "# 4.1 Setup evaluation metric - RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
        "\n",
        "# 4.2 Setup cross-validation\n",
        "n_folds = 5\n",
        "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# 4.3 Define base models\n",
        "models = {\n",
        "    'Lasso': Lasso(alpha=0.0005, random_state=42),\n",
        "    'ElasticNet': ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=42),\n",
        "    'Ridge': Ridge(alpha=15, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
        "                                               max_depth=4, max_features='sqrt',\n",
        "                                               min_samples_leaf=15, min_samples_split=10,\n",
        "                                               loss='squared_error', random_state=42),\n",
        "    'XGBoost': XGBRegressor(colsample_bytree=0.6, gamma=0.0,\n",
        "                            learning_rate=0.05, max_depth=4,\n",
        "                            min_child_weight=1.5, n_estimators=2500,\n",
        "                            reg_alpha=0.9, reg_lambda=0.6,\n",
        "                            subsample=0.8, random_state=42, verbosity=0),\n",
        "    'LightGBM': LGBMRegressor(objective='regression', num_leaves=5,\n",
        "                             learning_rate=0.05, n_estimators=1500,\n",
        "                             max_bin=55, bagging_fraction=0.8,\n",
        "                             bagging_freq=5, feature_fraction=0.2,\n",
        "                             feature_fraction_seed=9, bagging_seed=9,\n",
        "                             min_data_in_leaf=6, min_sum_hessian_in_leaf=11)\n",
        "}\n",
        "\n",
        "# 4.4 Evaluate models with cross-validation\n",
        "print(\"Model Evaluation with Cross-Validation:\")\n",
        "cv_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    cv_score = cross_val_score(model, train, y_train,\n",
        "                               scoring=rmse_scorer,\n",
        "                               cv=kf)\n",
        "    cv_results[name] = -np.mean(cv_score)  # Negative because our scorer is negative\n",
        "    print(f\"{name} - RMSE: {cv_results[name]:.4f}\")\n",
        "\n",
        "# 4.5 Stack models for better performance\n",
        "# Train all models\n",
        "trained_models = {}\n",
        "preds = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(train, y_train)\n",
        "    trained_models[name] = model\n",
        "    preds[name] = model.predict(train)\n",
        "\n",
        "# Build meta-model on predictions\n",
        "meta_X = np.column_stack([preds[name] for name in models.keys()])\n",
        "meta_model = Ridge(alpha=10)\n",
        "meta_model.fit(meta_X, y_train)\n",
        "\n",
        "# 4.6 Weighted averaging for final predictions\n",
        "model_weights = {\n",
        "    'Lasso': 0.2,\n",
        "    'ElasticNet': 0.1,\n",
        "    'Ridge': 0.1,\n",
        "    'GradientBoosting': 0.2,\n",
        "    'XGBoost': 0.2,\n",
        "    'LightGBM': 0.2\n",
        "}\n",
        "\n",
        "# Ensure weights sum to 1\n",
        "weight_sum = sum(model_weights.values())\n",
        "model_weights = {name: weight/weight_sum for name, weight in model_weights.items()}\n",
        "\n",
        "# ===============================\n",
        "# 5. Predictions and Submission\n",
        "# ===============================\n",
        "\n",
        "# 5.1 Make predictions on test data\n",
        "test_preds = {}\n",
        "for name, model in trained_models.items():\n",
        "    test_preds[name] = model.predict(test)\n",
        "\n",
        "# 5.2 Stack model predictions for test data\n",
        "meta_test_X = np.column_stack([test_preds[name] for name in models.keys()])\n",
        "stacked_pred = meta_model.predict(meta_test_X)\n",
        "\n",
        "# 5.3 Weighted average of predictions\n",
        "weighted_pred = np.zeros(test.shape[0])\n",
        "for name, weight in model_weights.items():\n",
        "    weighted_pred += test_preds[name] * weight\n",
        "\n",
        "# 5.4 Ensemble of stacked and weighted predictions\n",
        "final_pred = (stacked_pred + weighted_pred) / 2\n",
        "\n",
        "# 5.5 Transform predictions back to original scale\n",
        "final_pred = np.expm1(final_pred)\n",
        "\n",
        "# 5.6 Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_ID,\n",
        "    'SalePrice': final_pred\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file created!\")\n",
        "\n",
        "# ===============================\n",
        "# 6. Feature Importance\n",
        "# ===============================\n",
        "\n",
        "# 6.1 Get feature importance from GBM models\n",
        "gbm_importance = trained_models['GradientBoosting'].feature_importances_\n",
        "xgb_importance = trained_models['XGBoost'].feature_importances_\n",
        "\n",
        "# Create importance DataFrame\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': train.columns,\n",
        "    'GBM_Importance': gbm_importance,\n",
        "    'XGB_Importance': xgb_importance\n",
        "})\n",
        "\n",
        "importance_df['Avg_Importance'] = (importance_df['GBM_Importance'] +\n",
        "                                  importance_df['XGB_Importance']) / 2\n",
        "\n",
        "# Sort by average importance\n",
        "importance_df = importance_df.sort_values('Avg_Importance', ascending=False)\n",
        "\n",
        "# Plot top 20 features\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Avg_Importance', y='Feature', data=importance_df.head(20))\n",
        "plt.title('Top 20 Features by Importance')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Top 20 most important features:\")\n",
        "print(importance_df[['Feature', 'Avg_Importance']].head(20))\n",
        "\n",
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JovrSSl4RzK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
